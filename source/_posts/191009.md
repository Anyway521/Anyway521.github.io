---
title: Python re库
tags:
  - Python
  - 正则表达式
  - 笔记
  - 爬虫
toc: false
reward: true
declare: true
abbrlink: 3e6a6bf3
date: 2019-10-09 22:01:25
---

![u=3417224537,4087623395&fm=26&gp=0.jpg](https://cdn.anyway1314.cn/imageu=3417224537,4087623395&fm=26&gp=0.jpg-title)
<!-- ![QQ截图201tgds91009222648.png](https://cdn.anyway1314.cn/imageQQ截图201tgds91009222648.png-title) -->

Python学习笔记--Python-Re库( 正则表达式 )。

<!-- more -->

## 正则表达式常用操作符

![QQ截图201910vd08152325.png](https://cdn.anyway1314.cn/imageQQ截图201910vd08152325.png)

![QQ截图20191008152344.png](https://cdn.anyway1314.cn/imageQQ截图20191008152344.png)

## 经典实例

![QQ截图20191008152814.png](https://cdn.anyway1314.cn/imageQQ截图20191008152814.png)

## Re库的主要函数

![QQ截图20191009002524.png](https://cdn.anyway1314.cn/imageQQ截图20191009002524.png)

## re.search()
作用：搜索符合正则表达式的第一个对象  
``` python
import re
stri = 'fsijf 476600'
stru = '542111 faljfj'
strs = 'fakhfk 414444fauhf hfakh421111fhafh784444fahhfa475444'
zz = r'[1-9]\d{5}'

re.search
match = re.search(zz, stri)
if match:
    print(match.group(0))
```

## re.match()
作用：与re.search()类似,区别是re.match()会从每个字符串的开头进行匹配  
``` python
match2 = re.match(zz, stru)
if match2:
    print(match2.group(0))  
```

## re.findall() 
作用：搜索所有匹配表达式的子串,这里返回的是列表类型( 注意区别BeautifulSoup里面的soup.find_all() ) 
``` python
stra = re.findall(zz, strs)
print(stra,type(stra))
```
## re.split()
作用：将除了匹配的字符串外所有的部分分割  
``` python
strk = re.split(zz, strs)
print(strk)
strk = re.split(zz, strs,maxsplit=1)
#maxsplit指明了最大分割数，其余部分（不再进行匹配）直接全部输出
print(strk)
```
## re.finditer()
作用：返回一个匹配结果的迭代类型，可以用for循环遍历每个对象  
``` python
for it in re.finditer(zz, strs):
    if it:
        print(it.group(0)) 
```
## re.sub()  
作用：替换掉字符串里所有匹配的对象  
``` python
rp = "邮编" 
strrp = re.sub(zz, rp,strs)
print(strrp)
```
<b><p style = "color:red">以上是通俗用法，实际操作中，利用re.compile()更加方便</p></b>

## re.compile()
作用：将正则表达式的字符串形式编译成正则表达式对象
``` python
regex = re.compile(zz)
match = regex.findall(strs)
for i in match:
    print(i)
```
相比之前的通俗用法,将符合正则表达式的字符串编译后，拿这个对象继续去进行re的各类操作（search、match、findall、、、），更加简洁明了。

![QQ截图20191009004413.png](https://cdn.anyway1314.cn/imageQQ截图20191009004413.png)

## re库的match对象

``` python
import re
strs = 'fakhfk 414444fauhf hfakh421111fhafh784444fahhfa475444'
zz = r'[1-9]\d{5}'
regex = re.compile(zz) # 这里regex的类型是 ：<class 're.Pattern'>
m = regex.search(strs)
print(type(m)) # m的类型是：<class 're.Match'>
# m的各个属性
print(m.string)   # 待匹配对象字符串
print(m.re)       # 正则表达式
print(m.pos)      # 正则表达式搜索文本的开始位置
print(m.endpos)   # 正则表达式搜索文本的结束位置
print(m.start())  # 匹配字符串在原始字符串的开始位置
print(m.end())    # 匹配字符串在原始字符串的结束位置
print(m.span())   # 返回(start(),end())
print(m.group(0)) # 获得匹配后的字符串
```
程序输出结果：

![20191010000322.png](https://cdn.anyway1314.cn/image20191010000322.png)

## Re库默认采用贪婪匹配

![QQ截图20191009004137.png](https://cdn.anyway1314.cn/imageQQ截图20191009004137.png)

## 最小匹配

![QQ截图20191009004218.png](https://cdn.anyway1314.cn/imageQQ截图20191009004218.png)

## 最小匹配操作符

![QQ截图20191009004303.png](https://cdn.anyway1314.cn/imageQQ截图20191009004303.png)

## 附：爬虫实战-淘宝代码(2019-10-9可用)
注意这里淘宝需要自己账号先**登录**，然后添加header里面的`user-agent`以及`cookie`：浏览器`F12` -> `Network` -> `Doc` -> `headers`

![QQ截图20191009145037.png](https://cdn.anyway1314.cn/imageQQ截图20191009145037.png)

代码如下：

``` python
import requests
import re

def getHTMLText(url):
    header = {
        # 自己登录淘宝之后找
        'user-agent': '',
        'cookie':''
    }
    try:
        r = requests.get(url,headers=header,timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        # print(r.status_code)
        return r.text
    except:
        return "爬取错误"

def parsePage(ilt, html):
    try:
        plt = re.findall(r'\"view_price\"\:\"[\d\.]*\"', html)
        tlt = re.findall(r'\"raw_title\"\:\".*?\"', html)
        for i in range(len(plt)):
            price = eval(plt[i].split(':')[1])
            title = eval(tlt[i].split(':')[1])
            ilt.append([price, title])
    except:
        print("")        

def printGoodlist(ilt):
    tplt = "{:4}\t{:8}\t{:16}"
    print(tplt.format("序号", "价格", "名称"))
    count = 0
    for g in ilt:
        count = count + 1
        print(tplt.format(count,g[0],g[1]))

if __name__ == "__main__":
    
    depth = 2
    goods = "书包"
    start_url='https://s.taobao.com/search?q=' + goods
    indolist=[] #输出结果
    for i in range(depth):
        try:
            url=start_url + '&s=' + str(44*i)
            html=getHTMLText(url)
            parsePage(indolist,html)
        except:
            continue
    printGoodlist(indolist)

```


Ps:以上图片除标题外、均来自: 中国大学MOOC-Python网络爬虫与信息提取：<https://www.icourse163.org/course/BIT-1001870001>