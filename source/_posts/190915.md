---
title: VSCode + Anaconda 之 Scrapy 入门项目
tags:
  - Scrapy爬虫
  - VSCode
  - Anaconda
  - 笔记
abbrlink: c0592fb6
date: 2019-09-15 01:02:44
toc: true
reward: true
declare: true
---

![pyscrapy.jpg](https://cdn.anyway1314.cn/imagepyscrapy.jpg)

学习爬虫的第一个入门项目，顺便说说VSCode + Anaconda 配置  
Python Scrapy 的一些折磨人的坑。

<!-- more -->

# 开始搭建自己的项目
## 1、创建项目
这里我创建了一个`ScrapyProject`文件夹，用来存放各个Scrapy项目，要爬的网站是：`https://www.xicidaili.com/`

![QQ截图20190915140842.png](https://cdn.anyway1314.cn/imageQQ截图20190915140842.png)

所以项目命名为`xicidailiSpider`  
**cmd要cd到`ScrapyProject`目录下，然后执行**
``` python
    scrapy startproject xicidailiSpider
```
**这时候 scrapy 就会自动生成项目文件，注意下文件目录**  
`xicidailiSpider`文件夹下还有一个`xicidailiSpider`文件夹，以及一个`scrapy.cfg`配置文件。

![20190915140352.png](https://cdn.anyway1314.cn/image20190915140352.png)

打开`xicidailiSpider/xicidailiSpider`文件夹，可以看到这些文件，其中存放爬虫文件的就是`spiders`文件夹

![20190915141639.png](https://cdn.anyway1314.cn/image20190915141639.png)

## 2、生成爬虫文件
**cmd要cd到`ScrapyProject/xicidailiSpider`目录下，然后执行**
``` python
    scrapy genspider xicidaili xicidaili.com
    //释:    生成     爬虫名字   要爬的域名
```

![20190915144816.png](https://cdn.anyway1314.cn/image20190915144816.png)

这时候会发现`spiders`文件夹多了一个`xicidaili.py`，这就是我们要的爬虫文件

![20190915143737.png](https://cdn.anyway1314.cn/image20190915143737.png)

打开文件,一开始默认应该是这样的

![20190915145025.png](https://cdn.anyway1314.cn/image20190915145025.png)

准备爬点东西下来（`<tr>`内的`<td>`标签的内容）

![20190915144336.png](https://cdn.anyway1314.cn/image20190915144336.png)

修改`xicidaili.py`的内容为：

``` python
# -*- coding: utf-8 -*-
import scrapy


class XicidailiSpider(scrapy.Spider):
    name = 'xicidaili'
    allowed_domains = ['xicidaili.com']
    start_urls = ['https://www.xicidaili.com/nn/5']

    def parse(self, response):
        selectors = response.xpath('//tr')

        for selector in selectors:
            ip = selector.xpath('./td[2]/text()').get()
            port = selector.xpath('./td[3]/text()').get()
            print(ip,port)

```
Ps: 如何用正则表达式或者xpath()爬取内容需要自学  

## 3、运行爬虫文件
**这时候cmd还是在`ScrapyProject/xicidailiSpider`目录下，然后执行**
```
   scrapy crawl xicidaili
```

如图：

![20190915143458.png](https://cdn.anyway1314.cn/image20190915143458.png)

网站内容就爬下来了。

![20190915145224.png](https://cdn.anyway1314.cn/image20190915145224.png)

# 注意事项
## Anaconda的安装配置
### 安装时注意不要勾选第一个选项`Add Anaconda to my PATH enironment variable`（避免冲突）

![QQ截图20190912231241.png](https://cdn.anyway1314.cn/imageQQ截图20190912231241.png)

Ps: 为什么要这样，因为我之前加入之后，一旦运行爬虫文件或者其他安装命令时就会

![20190912170242.png](https://cdn.anyway1314.cn/image20190912170242.png)

遇到这个问题（无法输入程序输入点xxx于动态链接库上），可以有以下解决办法：  
1） 卸载重装该软件(对于Anaconda我就是这么做的,亲测有效)  
2） 安装微软常用运行库：[**点击跳转**](https://www.repaik.com/forum.php?mod=viewthread&tid=53948&extra=page%3D1%26filter%3Dtypeid%26typeid%3D168)

![20190915013107.png](https://cdn.anyway1314.cn/image20190915013107.png)

### 安装时选择`Just Me`就行

![QQ截图20190915012253.png](https://cdn.anyway1314.cn/imageQQ截图20190915012253.png)

### 手动设置系统环境变量
Ps: 这里我的安装路径是`C:\Users\24115\`
``` txt
    C:\Users\24115\Anaconda3
    C:\Users\24115\Anaconda3\Scripts
    C:\Users\24115\Anaconda3\Library\bin
    C:\Users\24115\Anaconda3\python.exe
```
### 用conda初始化cmd（之后在VSCode调试Scrapy需要设置默认终端cmd）
  
1）打开cmd,输入`conda init`  

![cmdinitconda.png](https://cdn.anyway1314.cn/imagecmdinitconda.png)

Ps: 之前初始化过，所以一堆`no change`.  
2）VSCode设置Conda Path  
打开VSCode配置文件，搜索`conda`

![QQ截图20190912234159.png](https://cdn.anyway1314.cn/imageQQ截图20190912234159.png)

这样做之后在VSCode底部打开终端时,cmd会自动初始化conda环境(系统忘记你就自己输`conda activate base`,罒ω罒)

![QQ截图20190915020330.png](https://cdn.anyway1314.cn/imageQQ截图20190915020330.png)

## Scrapy的一些坑

### 运行提示：`You are using pip version 10.0.1, however version 19.2.3 is available`
更新下pip
``` dos
    python -m pip install --upgrade pip
```
### 缺少依赖包
官方依赖下载网址：[**点击跳转**](https://www.lfd.uci.edu/~gohlke/pythonlibs/)
1）提示: `no module named pywin32`
直接搜`pywin32`,选择对应操作系统位数和python版本的whl文件下载（我这里是pywin32‑224‑cp38‑cp38‑win_amd64.whl）， 

![20190915015749.png](https://cdn.anyway1314.cn/image20190915015749.png)

用cmd打开到下载目录，执行
``` python
    pip install pywin32‑224‑cp38‑cp38‑win_amd64.whl
    //注意换成你的whl名字
```

2）提示: `twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.`
(同上网址)下载文件PyHamcrest(我这里是PyHamcrest‑1.9.0‑py2.py3‑none‑any.whl)  
用cmd打开到下载目录，执行
``` python
    pip install PyHamcrest‑1.9.0‑py2.py3‑none‑any.whl
    //注意换成你的whl名字
```

### 检测一下Scrapy是不是真的安装好了 
终端运行一下：
``` python
    scrapy bench
```
成功就会是下面的状况：

![20190915143115.png](https://cdn.anyway1314.cn/image20190915143115.png)

