---
title: Python 爬虫实战(3)-爬取CSDN博客信息
tags:
  - Python
  - 笔记
  - 爬虫
toc: false
reward: false
copyright: true
article_type: 0
abbrlink: a9966829
date: 2019-10-10 21:37:08
---

![QQ截图20191028205650_爱奇艺.jpg](https://cdn.anyway1314.cn/imageQQ截图20191028205650_爱奇艺.jpg-title)

Python爬虫实战-给定CSDN博主的主页URL，获取博主基本信息、文章信息。

<!-- more -->

## 我的CSDN主页：<https://blog.csdn.net/qq_40922859>

![QQ截图20191010224836.png](https://cdn.anyway1314.cn/imageQQ截图20191010224836.png)

这里我只爬取了博客左上角的基本信息，以及每篇文章的类型、标题、阅读数、原文链接等。

![GIF12.gif](https://cdn.anyway1314.cn/imageGIF12.gif)

直接上代码：

``` python
import requests
import re
from bs4 import BeautifulSoup

def getHTMLtext(url):
    header = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',
    }
    try:   
        r = requests.get(url, headers=header, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "爬取失败"


def getAuthorinfo(html):
    soup = BeautifulSoup(html, "html.parser")
    author_str = soup.select('#uid')
    list = []
    regex = re.compile('[\u4e00-\u9fa5]+')  # 匹配纯中文字符串
    # author = regex.findall(str(author_str))  # 不是所有人的名字都是纯中文,抛弃方案
    # if len(author):
    #     print("CSDN博主：{}".format(author[0]))
    print("------------------------------")
    print("博客信息如下：")
    print("CSDN博主：{}".format(str(author_str).split('\"')[7]))   #太难找了，而且格式不定，直接切片
    author_info1 = soup.select('#asideProfile > div.data-info.d-flex.item-tiling>dl')
    # print(type(author_info)) # list
    for info in author_info1:
        # print(type(info))   # tag
        if info.dt.string:
            print(info.dt.string + ': ' + info.dd.span.string)
            list.append(int(info.dd.span.string))
            #只需要第一个'原创数'，因为是循环里，所以直接全部保存，取第一个

    print("------------------------------")        
    author_info2 = soup.select('#asideProfile > div.grade-box.clearfix>dl')
    for info in author_info2:
        inl = info.dd.string
        if inl:
            bf =inl.replace(" ", "").replace("\n","")  #大喊一声py牛X
            print(info.dt.string+': '+bf)
        else:
            print(info.dt.string+': '+info.a.attrs.get('title')[0])
    xz = soup.select('#asideProfile > div.badge-box.d-flex > span')
    print("------------------------------")

    if regex.findall(str(xz)):         # 如果他有勋章
        print(regex.findall(str(xz))[0] + ":")
        author_info3 = soup.select('#asideProfile > div.badge-box.d-flex > div > div')
        for info in author_info3:
            print(info.attrs.get('title'))
        print("------------------------------")
    return list[0]    #返回原创文章数
    
def getperArticle(url,n):
    r = getHTMLtext(url)
    soup = BeautifulSoup(r, "html.parser")
    article_list = soup.select('#mainBox > main > div.article-list > div')
    count = 1
    tlp = "{:}-{:}、类型：{:4}\t标题：{} \t日期：{}\t阅读数：{:4}\t评论数：{:4}\n     原文链接：{}\n"
    for article in article_list:
        if article.h4:
            date = str(article.div.p.span.string).replace(" ", "").replace("\n","") 
            r = str(article.h4.a).split(' ', 6)
            temp =r[6]
            num = len(temp)
            R_P = re.findall('<span class="read-num">(.*?)</span>',str(article)) # 阅读数和评论数class都是read-num，真6
            print(tlp.format(n,count, article.h4.a.span.string, temp[6:num - 8], date,R_P[0].split('>')[1],R_P[1].split('>')[1],r[1][5:]))
            count = count+1
            

def getShortPage(num,url):
    # csdn的文章页数由js加载，现在不会爬，分析发现，一页有20篇文章，那大致就有"原创文章总数/20"页
    index = int(num / 20)
    url_temp = '/article/list/'
    if index:
        for i in range(1, index+1):
            url_st = url + url_temp + str(i) + '?'
            getperArticle(url_st,i)
    else:
        getperArticle(url,1)


if __name__ == "__main__":
    url=input('请输入你要查询的CSDN博客主页：')
    html = getHTMLtext(url)
    index = getAuthorinfo(html)
    getShortPage(index,url)
    
```

自己动手试试ヾ(◍°∇°◍)(￣▽￣)／  

Ps :2019-10-17、CSDN个人信息格式已经更改

![QQ截图20191017180832.png](https://cdn.anyway1314.cn/imageQQ截图20191017180832.png)

上述代码已经不能正常运行：

![QQ截图20191017180252.png](https://cdn.anyway1314.cn/imageQQ截图20191017180252.png)

无论是正常更新，还是特意反爬，这里都不再进行更新,代码仅做参考。

