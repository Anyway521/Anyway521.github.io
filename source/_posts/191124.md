---
title: 省内高校专业特色词频分析(单专业)
tags: Python
toc: true
reward: true
copyright: true
article_type: 0
abbrlink: cfbc4d4d
date: 2019-11-24 23:28:42
---

![timg1124e.jpg](https://cdn.anyway1314.cn/imagetimg1124e.jpg-titlew)

Python爬取高校专业特色文件，进行词频分析。

<!-- more -->

## 总体思路：
先根据自己要分析的专业，去找开了这个专业的学校信息，找到这些学校里这个专业的`专业特色`文件并下载（PDF文件），然后PDF转文本，用Python的`jieba库`进行词汇分析。（看不懂可以先忽略）

## 完整代码
``` python  
""" 这里用了很多库，需要自己用pip安装
pip install pyocr
pip install pdfminer3k
pip install requests
pip install jieba 
这些可能还不够，一般来说，运行代码提示‘No module named xxx’,就是要自己 pip install xxx
"""
import pyocr,importlib,sys,time,os,re,requests,jieba
from urllib.request import urlopen   # 用来从url直接打开PDF
from pdfminer.pdfparser import PDFParser,PDFDocument
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import PDFPageAggregator
from pdfminer.layout import LTTextBoxHorizontal,LAParams
from pdfminer.pdfinterp import PDFTextExtractionNotAllowed

importlib.reload(sys)

def GetHTMLText(url):   #爬取url内容
    header = {
        'user-agent': "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36",
    }
    try:
        r = requests.get(url, headers=header, timeout=10)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "爬取超时"

def GetUrlList(html):  # 河南省有该专业的所有学校
    # regex = re.compile(r'\..+(html)') # 坑，带括号就只匹配括号里的东西
    regex = re.compile(r'\..+\.html')
    url_pre = 'http://zypt.neusoft.edu.cn/hasdb/pubfiles/gongshi2017/'
    url_list = []
    url_all = regex.findall(html,re.M)
    # url_all = regex.finditer(html,re.M)
    for url_nif in url_all:
        # print(url_nif)
        if len(url_nif) > 32:
            temp = url_pre + url_nif[3:28] + url_nif[15:28] + '_TK_ZYTS.html'
            # print(temp)
            url_list.append(temp)
    return set(url_list)

def GetPDFList(url_list):  #爬取PDF的URL
    pdf_list = []
    for url in url_list:
        html = GetHTMLText(url)
        regex = re.compile(r'\.\..+.pdf')
        pdf_url_nif = regex.search(html).group(0)
        pdf_url_pre = 'http://zypt.neusoft.edu.cn/hasdb/'
        pdf_url = pdf_url_pre + pdf_url_nif[15:]
        pdf_list.append(pdf_url)
        # print(pdf_url)
    return pdf_list


def parse(pdf_url):
    '''解析PDF文本，并保存到TXT文件中'''
    # fp = open(text_path,'rb')
    fp = urlopen(pdf_url)
    #用文件对象创建一个PDF文档分析器
    parser = PDFParser(fp)
    #创建一个PDF文档
    doc = PDFDocument()
    #连接分析器，与文档对象
    parser.set_document(doc)
    doc.set_parser(parser)
 
    #提供初始化密码，如果没有密码，就创建一个空的字符串
    doc.initialize()
 
    #检测文档是否提供txt转换，不提供就忽略
    if not doc.is_extractable:
        raise PDFTextExtractionNotAllowed
    else:
        #创建PDF，资源管理器，来共享资源
        rsrcmgr = PDFResourceManager()
        #创建一个PDF设备对象
        laparams = LAParams()
        device = PDFPageAggregator(rsrcmgr,laparams=laparams)
        #创建一个PDF解释其对象
        interpreter = PDFPageInterpreter(rsrcmgr,device)
 
        #循环遍历列表，每次处理一个page内容
        # doc.get_pages() 获取page列表
        for page in doc.get_pages():
            interpreter.process_page(page)
            #接受该页面的LTPage对象
            layout = device.get_result()
            # 这里layout是一个LTPage对象 里面存放着 这个page解析出的各种对象
            # 一般包括LTTextBox, LTFigure, LTImage, LTTextBoxHorizontal 等等
            # 想要获取文本就获得对象的text属性，
            for x in layout:
                # a+：追加方式打开，把所有PDF的文字都写到一个TXT里
                if(isinstance(x,LTTextBoxHorizontal)):
                    with open(r'cpfx.txt','a+',encoding='utf-8') as f:
                        try:
                            results = x.get_text()
                        # print(results)
                        except UnicodeEncodeError:
                            print("编码出错")
                        else:    
                            f.write(results  +"\n")


def PDFtoTEXT(pdf_list): # 把所有PDF都转存到一个txt里
    for pdf_url in pdf_list:
        parse(pdf_url)

def CPFX():   # 利用jieba库进行词频分析
    txt = open(r'cpfx.txt', 'r', encoding='utf-8').read()
    words = jieba.lcut(txt)
    counts = {}
    for word in words:
        if len(word) == 1:
            continue
        else:
            counts[word] = counts.get(word, 0) + 1
    # 筛选需要剔除的无关词
    excludes = {'专业', "学生", "化学", "9.1", "教师", "特色", "河南省", "支撑"}
    for word in excludes:
        del (counts[word])
    items = list(counts.items())
    items.sort(key=lambda x: x[1], reverse=True)
    for i in range(20):
        word, count = items[i]
        print("{0:<10}{1:<5}".format(word, count))


if __name__ == "__main__":
    index = 'http://zypt.neusoft.edu.cn/hasdb/pubfiles/gongshi2017/spec/070301.html'
    r = GetHTMLText(index)
    url_list = GetUrlList(r)
    pdf_list = GetPDFList(url_list)
    PDFtoTEXT(pdf_list)
    CPFX()
```

## 实现过程
先来到(浏览器打开)这个页面：<http://zypt.neusoft.edu.cn/hasdb/pubfiles/gongshi2017/index.html>  

![QQ截图20191124234535.png](https://cdn.anyway1314.cn/imageQQ截图20191124234535.png)

按专业查找，找到自己需要分析的专业的代号。比如我要分析的是化学的，代号是`070301`  

![20191124234816.png](https://cdn.anyway1314.cn/image20191124234816.png)

python文件main函数的 index 就是`http://zypt.neusoft.edu.cn/hasdb/pubfiles/gongshi2017/spec/`+`代号.html` (这里改成你要分析的专业代号就行了，其他的全都不用改)  
比如：化学专业的主页就是：  

![20191124235505.png](https://cdn.anyway1314.cn/image20191124235505.png)

浏览器打开：

![QQ截图20191125001204.png](https://cdn.anyway1314.cn/imageQQ截图20191125001204.png)

看看这些大学的链接：

![20191125002617.png](https://cdn.anyway1314.cn/image20191125002617.png)

浏览器选一个大学点进去，比如我这选的郑大：

![QQ截图20191125003536.png](https://cdn.anyway1314.cn/imageQQ截图20191125003536.png)

我们需要的是每个大学的 `9.1 专业特色、实施过程和效果说明` 里面的文件

![20191125003814.png](https://cdn.anyway1314.cn/image20191125003814.png)

而 `9.1 专业特色、实施过程和效果说明` 的链接都是这样的

![20191125010033.png](https://cdn.anyway1314.cn/image20191125010033.png)

所以爬取这些URL的时候需要进行拼接：

``` python
# 爬取所有 `9.1 专业特色、实施过程和效果说明` 的链接
def GetUrlList(html):  
    # 正则，匹配<a>标签里的那些内容
    regex = re.compile(r'\..+\.html')
    url_pre = 'http://zypt.neusoft.edu.cn/hasdb/pubfiles/gongshi2017/'
    url_list = []
    url_all = regex.findall(html,re.M)
    for url_nif in url_all:
        if len(url_nif) > 32:
            # 拼接URL
            temp = url_pre + url_nif[3:28] + url_nif[15:28] + '_TK_ZYTS.html'
            url_list.append(temp)
    return set(url_list)
```

打开其中一个学校的 `9.1 专业特色、实施过程和效果说明` 页面，可以找到我们最终需要的PDF文件地址。

![QQ截图20191125004551.png](https://cdn.anyway1314.cn/imageQQ截图20191125004551.png)

对应 `GetPDFList()` 函数，爬取这些PDF的URL。
``` python
def GetPDFList(url_list):  #爬取PDF的URL
    pdf_list = []
    for url in url_list:
        html = GetHTMLText(url)
        regex = re.compile(r'\.\..+.pdf')
        pdf_url_nif = regex.search(html).group(0)
        pdf_url_pre = 'http://zypt.neusoft.edu.cn/hasdb/'
        pdf_url = pdf_url_pre + pdf_url_nif[15:]
        pdf_list.append(pdf_url)
    return pdf_list
```
拿到这些URL,就可以把PDF下载下来，转成TXT文本。
``` python
def parse(pdf_url):
    # fp = open(text_path,'rb')
    fp = urlopen(pdf_url)
    #用文件对象创建一个PDF文档分析器
    parser = PDFParser(fp)
    #创建一个PDF文档
    doc = PDFDocument()
    #连接分析器，与文档对象
    parser.set_document(doc)
    doc.set_parser(parser)
 
    #提供初始化密码，如果没有密码，就创建一个空的字符串
    doc.initialize()
 
    #检测文档是否提供txt转换，不提供就忽略
    if not doc.is_extractable:
        raise PDFTextExtractionNotAllowed
    else:
        #创建PDF，资源管理器，来共享资源
        rsrcmgr = PDFResourceManager()
        #创建一个PDF设备对象
        laparams = LAParams()
        device = PDFPageAggregator(rsrcmgr,laparams=laparams)
        #创建一个PDF解释其对象
        interpreter = PDFPageInterpreter(rsrcmgr,device)

        for page in doc.get_pages():
            interpreter.process_page(page)
            layout = device.get_result()
            for x in layout:
                if (isinstance(x, LTTextBoxHorizontal)):
                    # a+：追加方式打开，把所有PDF的文字都写到cpfx.txt里
                    with open(r'cpfx.txt','a+',encoding='utf-8') as f:
                        try:
                            results = x.get_text()
                        
                        except UnicodeEncodeError:
                            print("编码出错")
                        else:    
                            f.write(results  +"\n")


def PDFtoTEXT(pdf_list): # 把所有PDF都转存到cpfx.txt里
    for pdf_url in pdf_list:
        parse(pdf_url)
```

最后用jieba库进行分析
``` py
def CPFX():   # 利用jieba库进行词频分析
    txt = open(r'cpfx.txt', 'r', encoding='utf-8').read()
    words = jieba.lcut(txt)
    counts = {}
    for word in words:
        if len(word) == 1:
            continue
        else:
            counts[word] = counts.get(word, 0) + 1
    # 筛选需要剔除的无关词
    excludes = {'专业', "学生", "化学", "9.1", "教师", "特色", "河南省", "支撑"}
    for word in excludes:
        del (counts[word])
    items = list(counts.items())
    items.sort(key=lambda x: x[1], reverse=True)
    for i in range(20):
        word, count = items[i]
        print("{0:<10}{1:<5}".format(word, count))
```
完结、撒花✿✿ヽ(°▽°)ノ✿  . But、一个疑问贴在这里：
## 为什么re在进行正则匹配时、表达式带括号会给出意外的结果？
带括号就只匹配括号里的内容  

![带括号就.png](https://cdn.anyway1314.cn/image带括号就.png)

但是带括号的，用 `re` 的 `finditer` 实际上可以匹配到正确结果

![iter发现能找出来.png](https://cdn.anyway1314.cn/imageiter发现能找出来.png)

最后还是去掉括号才能正确匹配。

![QQ截图20191120235900.png](https://cdn.anyway1314.cn/imageQQ截图20191120235900.png)

Why???