---
title: 初识Python BeautifulSoup库
tags:
  - Python
  - 笔记
  - 爬虫
toc: false
reward: false
copyright: true
article_type: 0
abbrlink: 3d87cef4
date: 2019-10-08 00:52:28
---

![QQ截图20191008010142.png](https://cdn.anyway1314.cn/imageQQ截图20191008010142.png-titlew)

简单了解下Python爬虫中用到的BeautifulSoup4库。

<!-- more -->


# BeautifulSoup类的基本元素
BeautifulSoup的作用就是：把从网上获取的HTML文本，处理成一种对象，便于更好地对它进行操作。

|基本元素|说明|
|:---|:---|
|Tag|Html标签：形如<>...</>|
|Name|标签名：`<body>...</body>`, 格式：Tag.name = 'body'|
|Attributes|标签的属性：比如a标签里的 class,href..., 格式：Tag.attrs|
|NavigableString|标签内的字符串：形如<>字符串</>，格式：Tag.string|
|Comment|标签内字符串的注释部分|

## 1、`BeautifulSoup`的引入
第一行是只引入BeautifulSoup类以及相关操作，第二行则是引入整个bs4库。
``` python
from bs4 import BeautifulSoup
import bs4
```

## 2、HTML文本对象转换为`BeautifulSoup`类
``` python
import requests
from bs4 import BeautifulSoup

url = ''
html = requests.get(url)
soup = BeautifulSoup(html.text,"html.parser")
```

## 3、`BeautifulSoup`类各元素的类型
这里执行一下：
``` python
soup = BeautifulSoup(html.text,"html.parser")
print(type(soup))
```
结果是：![20191008135427.png](https://cdn.anyway1314.cn/image20191008135427.png)

例如要查找HTML里的所有`<link>`标签：

![20191008141959.png](https://cdn.anyway1314.cn/image20191008141959.png)

这里可以用：`soup.find_all('link')` 或者 `soup('link')`

``` python
links = soup.find_all('link')
# 等价于links = soup('link')
print(type(links))
```
返回的结果是一个结果集合(包含所有的`<link>`标签)：![20191008142452.png](https://cdn.anyway1314.cn/image20191008142452.png) 可以用for循环打印出来：
``` python
for link in links:
    print(link)

print(type(link))     #这里看一下 link 的类型：
```
已经是bs4的`Tag`类型了：![20191008142817.png](https://cdn.anyway1314.cn/image20191008142817.png)
如果我们想获取`<link>`标签里的链接( href="houtai/templates/images/favicon.png" ),就需要进一步使用`Tag`的`attrs`
``` python
for link in links:
    print(link.attrs)    # 打印每个link标签的attribute
    print(type(link.attrs))
```
执行结果为：

![20191008144408.png](https://cdn.anyway1314.cn/image20191008144408.png)

`<link>`标签中有很多属性：比如：
``` html
<link rel="shortcut icon" href="houtai/templates/images/favicon.png" />
```
这里的`<link>`标签属性就有`rel`、`href`，他们都在一个字典里。可以通过字典的方式访问。
``` python
for link in links:
    print(link.attrs.get('href'))  # 打印出每个link里的href属性
    # 等价于 print(link.attrs['href'])
```
总结如下图：

![20191008150257.png](https://cdn.anyway1314.cn/image20191008150257.png)

附：爬取中国大学排名网2019大学排名代码
``` python
import requests
import bs4
from bs4 import BeautifulSoup

def getHTMtext(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "爬取失败"

def fillUnvilist(ulist, html):
    soup = BeautifulSoup(html, "html.parser")
    for tr in soup.find('tbody').children:
        if isinstance(tr, bs4.element.Tag):   #判断一下是不是Tag
           tds = tr('td')
           # 其实就是 tds = tr.find_all('td')
           ulist.append([tds[0].string, tds[1].string, tds[2].string])

def printUnvlist(ulist, num):
    tplit = "{0:^10}\t{1:{3}^10}\t{2:^10}"
    # 默认的补齐使用英文空格（占一个字符），对中文（两个字符）格式上不友好，这里改成补齐用中文空格（2个字符）
    print(tplit.format("排名", "学校名称", "总分", chr(12288)))
    #  .format里四个参数对应tplit的0、1、2、3
    for i in range(num):
        u = ulist[i]
        print(tplit.format(u[0],u[1],u[2],chr(12288)))

if __name__ == "__main__":
    uinfo = []
    url = 'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html'
    html = getHTMtext(url)
    fillUnvilist(uinfo, html)
    printUnvlist(uinfo, 20)
```
人生苦短，我用Pythonヾ(◍°∇°◍)ﾉﾞ